{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8066917,"sourceType":"datasetVersion","datasetId":4759383}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport torch\nfrom datasets import Dataset\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling\nfrom transformers import Trainer, TrainingArguments\n\n# Load pre-trained GPT-2 model and tokenizer\nmodel_name = \"gpt2\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\ntokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:19:21.249775Z","iopub.execute_input":"2024-04-09T06:19:21.250489Z","iopub.status.idle":"2024-04-09T06:19:44.651070Z","shell.execute_reply.started":"2024-04-09T06:19:21.250456Z","shell.execute_reply":"2024-04-09T06:19:44.650205Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-04-09 06:19:31.691163: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-09 06:19:31.691311: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-09 06:19:31.819492: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ae3194df9854fe0938ca0c80f0bbc5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5457ba2556f64a77a2857a9a1c1bdca7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d5de664a67f49ae93c23b7925943dff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5ee35cf6fa9452b9eb0151d4fc3c02e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6090d01e02c408aaf4138e374f56c45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9c4c956fd394b82916e22d87e92ed6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06f2c99760e647ebb430f5579f5146ff"}},"metadata":{}}]},{"cell_type":"code","source":"# Load prompt and response data from JSON file\nfrom sklearn.model_selection import train_test_split\n\nfile_path = \"/kaggle/input/commands/dataset.json\"\nwith open(file_path, \"r\", encoding=\"utf-8\") as file:\n    data = json.load(file)\n\n\n# Extract prompts and commands\nprompts = [item['prompt'] for item in data]\ncommands = [item['command'] for item in data]\n\n# Create a dictionary\ndata_dict = {\n    'prompt': prompts,\n    'command': commands\n}\n\n# train, test = train_test_split(data,test_size=0.15)\n# print(train)\n\n# train_dataset = Dataset.from_dict(train)\n# test_dataset = Dataset.from_dict(test)\n# print(train_dataset)\n\n# Create Dataset object\ndataset = Dataset.from_dict(data_dict)\nprint(dataset[0])\n","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:48:43.700038Z","iopub.execute_input":"2024-04-09T06:48:43.700967Z","iopub.status.idle":"2024-04-09T06:48:43.719299Z","shell.execute_reply.started":"2024-04-09T06:48:43.700932Z","shell.execute_reply":"2024-04-09T06:48:43.718426Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"{'prompt': 'List files and directories in the current directory', 'command': 'ls'}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Tokenize the dataset\ndef tokenize_function(example):\n    return tokenizer(example[\"prompt\"], example[\"command\"], padding=True, truncation=True)\n\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\ntokenized_dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:48:59.769907Z","iopub.execute_input":"2024-04-09T06:48:59.770231Z","iopub.status.idle":"2024-04-09T06:49:02.077719Z","shell.execute_reply.started":"2024-04-09T06:48:59.770207Z","shell.execute_reply":"2024-04-09T06:49:02.076453Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/179 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a24899e22949485da7101996a939d592"}},"metadata":{}},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'command', 'input_ids', 'attention_mask'],\n    num_rows: 179\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Define data collator\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:49:05.349556Z","iopub.execute_input":"2024-04-09T06:49:05.350203Z","iopub.status.idle":"2024-04-09T06:49:05.355893Z","shell.execute_reply.started":"2024-04-09T06:49:05.350173Z","shell.execute_reply":"2024-04-09T06:49:05.354708Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Fine-tuning arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./finetuned_gpt2\",\n    overwrite_output_dir=True,\n    num_train_epochs=5,\n    per_device_train_batch_size=4,\n    save_steps=800,\n    save_total_limit=2,\n)\n\n# Define trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=tokenized_dataset,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:50:29.114351Z","iopub.execute_input":"2024-04-09T06:50:29.114747Z","iopub.status.idle":"2024-04-09T06:50:29.268101Z","shell.execute_reply.started":"2024-04-09T06:50:29.114713Z","shell.execute_reply":"2024-04-09T06:50:29.267196Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Fine-tune the model\ntrainer.train()\n\n# Save the fine-tuned model\ntrainer.save_model(\"./finetuned_gpt2\")\n\n\n# Save the tokenizer\ntokenizer.save_pretrained(\"./finetuned_gpt2\")","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:50:32.999269Z","iopub.execute_input":"2024-04-09T06:50:32.999636Z","iopub.status.idle":"2024-04-09T06:50:48.189589Z","shell.execute_reply.started":"2024-04-09T06:50:32.999607Z","shell.execute_reply":"2024-04-09T06:50:48.188691Z"},"trusted":true},"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [225/225 00:13, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"('./finetuned_gpt2/tokenizer_config.json',\n './finetuned_gpt2/special_tokens_map.json',\n './finetuned_gpt2/vocab.json',\n './finetuned_gpt2/merges.txt',\n './finetuned_gpt2/added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Load fine-tuned GPT-2 model and tokenizer\nmodel_name = \"./finetuned_gpt2\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:50:51.685006Z","iopub.execute_input":"2024-04-09T06:50:51.685720Z","iopub.status.idle":"2024-04-09T06:50:51.933008Z","shell.execute_reply.started":"2024-04-09T06:50:51.685688Z","shell.execute_reply":"2024-04-09T06:50:51.932030Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Define the prompt\nprompt = \"create a directory named 'test'\"\n\n# Tokenize the prompt\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n# Generate response\noutput = model.generate(input_ids, max_length=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T07:55:00.424144Z","iopub.execute_input":"2024-04-09T07:55:00.424489Z","iopub.status.idle":"2024-04-09T07:55:00.752713Z","shell.execute_reply.started":"2024-04-09T07:55:00.424462Z","shell.execute_reply":"2024-04-09T07:55:00.751590Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate a directory named \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Tokenize the prompt\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Generate response\u001b[39;00m\n\u001b[1;32m      8\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(input_ids, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, pad_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id)\n","\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"],"ename":"NameError","evalue":"name 'tokenizer' is not defined","output_type":"error"}]},{"cell_type":"code","source":"print(output[0])","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:34:11.074482Z","iopub.execute_input":"2024-04-09T06:34:11.075213Z","iopub.status.idle":"2024-04-09T06:34:11.083648Z","shell.execute_reply.started":"2024-04-09T06:34:11.075183Z","shell.execute_reply":"2024-04-09T06:34:11.082535Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"tensor([27914,   262,  2393,  3706,   705, 20688,    13, 14116,     6, 26224,\n         1672,    13, 14116,  1672,    13, 14116,   930,   266,    66,   532,\n           75,  1672,    13, 14116,   930,  7894,   532,    77,   352,   930,\n         7894,   532,    77,   352,   930,   266,    66,   532,    75,  1672,\n           13, 14116,   930,  7894,   532,    77,   352,   930,  7894,   532])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Decode the generated response\n\n\nresponse = tokenizer.decode(output[0], skip_special_tokens=True)\n\n# # Print the response\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:51:04.049287Z","iopub.execute_input":"2024-04-09T06:51:04.049657Z","iopub.status.idle":"2024-04-09T06:51:04.060671Z","shell.execute_reply.started":"2024-04-09T06:51:04.049627Z","shell.execute_reply":"2024-04-09T06:51:04.059529Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Remove the file named 'example.txt'rm example.txt example.txt | wc -l example.txt | tail -n 1 | wc -l example.txt | tail -n 1 | tail -n 1 | wc\n","output_type":"stream"}]}]}